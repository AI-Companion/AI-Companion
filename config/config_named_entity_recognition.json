{
    "dataset_name":"kaggle_ner",
    "model_name":"rnn",
    "__help_model_name":"can be either 'rnn' or 'tfidf'",
    "dataset_url":"data/ner_dataset.zip",
    "__help_dataset_url":"url for the specified dataset",
    "experimental_mode":false,
    "__help_experimental_mode":"set to true if you just looking for a 'quick and dirty model'",
    "batch_size":64,
    "__help_batch_size":"used to train the model, must be power of 2",
    "validation_split":0.15,
    "__help_validation_split":"train test split ratio",
    "max_sequence_length":75,
    "__help_max_sequence_length":"maximum length of training features",
    "embeddings_path_glove":"http://nlp.stanford.edu/data/glove.6B.zip",
    "__help_embeddings_path_glove":"url for glove embedding file",
    "embeddings_path_fasttext":"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip",
    "__help_embeddings_path_fasttext":"url for fasttext embedding file",
    "embedding_algorithm":"fasttext",
    "__help_embedding_algorithm":"can be glove or fasttext, else an empty embedding layer will be used",
    "embedding_dimension":100,
    "__help_embedding_dimension":"glove possible embedding dimensions are [50, 100, 200, 300] any other value will be replaced by 100, if embedding is fasttext then this parameter is ignored and 300 is applied",
    "vocab_size":35000,
    "__help_vocab_size":"count of unique tokens",
    "pre_trained_embedding":true,
    "__help_pre_trained_embedding":"set to true if you want to use a pre-trained embedding, otherwise the model will learn the embedding",
    "h5_model_url":"148OeDlWhoGc0dGSxz5nGEvXz3PJiCGb5",
    "_help_h5_model_url":"gdrive url for the trained h5 model",
    "class_file_url":"1Sw-UHBs6VHiCOUbmgsP3eFUCHM3pAYVK",
    "_help_class_file_url":"gdrive url for the trained class file",
    "preprocessor_file_url":"1krWj5UzzbM5ROkRAcZFTOzzDTIjlmEo3",
    "_help_preprocessor_file_url":"gdrive url for the trained preprocessor file"
}